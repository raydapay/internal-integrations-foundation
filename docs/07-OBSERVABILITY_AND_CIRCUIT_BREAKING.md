# Architectural Decision Record 07: Observability & Circuit Breaking

> **"Degrade gracefully. Alert deterministically. Never fail silently."**

This document formalizes the telemetry, alerting, and automated circuit-breaking mechanisms within the Integration Gateway. As the system scales to support complex, highly mutable target schemas (e.g., dynamic Jira fields), it must transition from a reactive error-logging posture to a proactive self-healing and isolation posture.

## 1. The Telemetry & Health Subsystem

To prevent the Gateway from operating blindly during infrastructure degradation, we implement a periodic, self-evaluating health check executed by the ARQ worker pool.

### 1.1 Dynamic Threshold Constraints
Systemic health boundaries are not hardcoded; they are defined in the mutable `DomainConfig` SQLite table to allow runtime tuning without deployment cycles.
* **Hardware Utilization:** The application utilizes `psutil` to monitor host memory and root partition disk space. The default critical threshold is established at `90.0%`.
* **Worker Starvation:** The engine measures ARQ backlog via Redis (`zcard`). The default queue depth threshold is `500` pending jobs. Exceeding this indicates worker death, I/O blocking, or catastrophic downstream rate-limiting.
* **Network Integrity:** The system performs asynchronous HTTP pings to the PeopleForce, Jira, Slack, and Telegram APIs to detect network partitions.

## 2. Notification Fallback Matrix

Alerts generated by the health subsystem or catastrophic worker crashes must reach human operators. Relying on a single external webhook introduces a single point of failure in the observability chain.

* **Primary Channel:** Slack Webhook.
* **Reserve Channel:** Telegram Bot API.
* **Execution Logic:** The `src.core.notifications.notify` dispatcher attempts the primary channel first. If the HTTP request times out or returns a `4xx/5xx` status, it catches the exception and immediately falls back to the reserve channel.

## 3. Proactive Validation & Circuit Breaking (Schema Protection)

SaaS platforms like Jira present a "moving target" challenge. Administrators frequently alter field configurations, required status, and allowed enum values. The engine must protect itself from entering infinite `O(N)` retry loops when encountering static mapping failures caused by upstream schema drift.

### 3.1 Nightly Validation Sync
The system will run a proactive `validate_routing_rules_task` via an ARQ cron schedule during periods of low locking contention (e.g., 03:00 UTC). This task simulates payload generation and validates the `RoutingRule` targets against Jira's real-time `createmeta` schema.

### 3.2 The Circuit Breaker Protocol
If the validation task detects an irreconcilable schema drift (e.g., an unmapped required field, or a deleted project key), the engine executes the following protocol:
1.  **Isolate:** The specific mapping rule is automatically disabled via a database mutation (`UPDATE routingrule SET is_active = False WHERE id = ?`).
2.  **Alert:** A critical alert is dispatched via the Notification Matrix detailing the specific rule ID and the missing Jira requirement.
3.  **Yield:** Future Webhook/Sync events matching the disabled rule will bypass the Jira API entirely, yielding execution until an administrator resolves the mapping delta and manually reactivates the rule.

**Assumption:** Temporary API timeouts during the nightly sync could trigger false positives. The validation task must implement an exponential backoff retry mechanism (e.g., 1s, 2s, 4s, 8s, 16s) before officially tripping the circuit breaker and disabling a rule.

### 3.3 Reactive Cache Invalidation (The 400 Trap)
The `FieldDataResolver` caches Jira `createmeta` schemas in Redis with a 1-hour TTL (`jira:createmeta:{project}:{issuetype}`) to prevent N+1 API rate limiting. This introduces a vulnerability window: if a Jira administrator alters a schema, the local cache becomes stale until the TTL expires or the Nightly Sync runs.

**The Self-Healing Mechanism:**
To bridge this gap, the ARQ worker wraps Jira `POST/PUT` mutations in an HTTP trap.
1. If Jira rejects the payload with an `HTTP 400 Bad Request`, the worker assumes schema drift.
2. It surgically deletes the specific project/issuetype schema key from Redis.
3. It raises an explicit ARQ `Retry(defer=5)` exception.
4. On the subsequent execution 5 seconds later, the resolver fetches the fresh schema directly from Jira, evaluates the new constraints, and cleanly trips the `SchemaValidationError` circuit breaker, neutralizing the Poison Pill payload.

## 4. Architectural Forecasts & Trade-offs

When forecasting the impact of proactive schema validation on operational workflows, we evaluate two systemic trajectories:

* **Forecast A (Hard Circuit Breaker):** The system disables the broken rule immediately and drops matching events.
  * *Trade-off:* Achieves absolute data consistency and zero API waste. However, requires high administrative overhead, as operators must manually replay missed tasks after fixing the mapping.
* **Forecast B (Soft-Disable via Dead Letter Queue):** The system keeps the rule active but routes subsequent matching events into a persistent Staging Queue / DLQ.
  * *Trade-off:* Prevents data loss and operational panic, but introduces significant architectural complexity. Requires expanding the SQLite schema to track deferred payloads and building an HTMX UI to batch-replay events once the schema is corrected.